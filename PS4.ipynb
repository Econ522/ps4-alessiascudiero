{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9803c92c-fec5-42b0-95dc-04f4147b16c2",
   "metadata": {},
   "source": [
    "# Problem Set 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff25f34-87db-4e82-991f-285541111b56",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aa8c39-0c90-4db3-ab56-62c754be4930",
   "metadata": {},
   "source": [
    "In this exercise, we numerically study the behaviors of the wild and the pairs bootstrap by considering the following model:\n",
    "\\begin{align}\\label{Q4}\\tag{1}\n",
    "Y_i=Z_{1i} + Z_{2i} + Z_{3i} + \\varrho_0 Z_{1i} Z_{2i} + (1+\\lambda_0 Z_{1i})\\epsilon_i~,\n",
    "\\end{align}\n",
    "for $i=1,\\ldots, n$. The sample $\\{Y_i,Z_{1i},Z_{2i},Z_{3i}\\}_{i=1}^n$ is generated as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3208f85b-cec8-4490-96dc-baea914e0c34",
   "metadata": {},
   "source": [
    "(1) Generate $V_i,Z_{2i}$ and $Z_{3i}$ from the standard normal distribution with equal correlation $0.2$, and then set\n",
    "    \\begin{align}\n",
    "    Z_{1i} = \\frac{\\exp(V_i)-E[\\exp(V_i)]}{\\sqrt{\\mathrm{Var}(\\exp(V_i))}}~,\n",
    "    \\end{align}\n",
    "    where $E[\\exp(V_i)]=\\exp(0.5)$ and $\\sqrt{\\mathrm{Var}(\\exp(V_i))}=2.16$ (approximately).\n",
    "\n",
    "(2) Independently, $\\epsilon_i$ is generated from a mixture of a $N(-1/9,1)$ variable with probability $0.9$ and a $N(1,4)$ variable with probability $0.1$. When coding, you may first draw a sample $\\eta_i$ from a Bernoulli distribution with success probability $0.9$, and then you draw $\\epsilon_i$ from $N(-1/9,1)$ if $\\eta_i=1$ and otherwise draw $\\epsilon_i$ from $N(1,4)$. There may be a Python package for implementing this in a compact fashion, but, if not, I would just set $\\epsilon_i=\\eta_i W_{1i} + (1-\\eta_i)W_{2i}$ for $W_{1i}\\sim N(-1/9,1)$ and independently $W_{2i}\\sim N(1,4)$.\n",
    "\n",
    "(3) Generate $Y_i$ according to (1) for each pair $(\\varrho_0,\\lambda_0)$ to be specified.\n",
    "\n",
    "Given $\\{Y_i,Z_{1i},Z_{2i},Z_{3i}\\}_{i=1}^n$, we then consider the following regression:\n",
    "  \\begin{align}\n",
    "  Y_i=\\alpha_0+\\beta_1Z_{1i} + \\beta_2 Z_{2i} + \\beta_3 Z_{3i} + U_i~.\n",
    "  \\end{align}\n",
    "Note that if $\\varrho_0=0$, then $E[U_i|Z_{1i},Z_{2i},Z_{3i}]=0$ (the model is correctly specified), and if $\\varrho_0\\neq 0$, then $E[U_i|Z_{1i},Z_{2i},Z_{3i}]\\neq 0$ so the model is misspecified (but one still has $E[U_i(1,Z_{1i},Z_{2i},Z_{3i})]=0$). The model exhibits conditional heteroskedasticity if $\\lambda_0\\neq 0$. In what follows, set $\\lambda_0=1$ and $n=200$.\n",
    "\n",
    " You should manually generate the bootstrap samples and compute the bootstrap statistics, as described on p.51 of the lecture notes.\n",
    "\n",
    "(a) Let $\\varrho_0=0$. Compute the empirical rejection rates of the two-sided $t$-test for $\\mathrm H_0: \\beta_1=1$ at the significance level $\\alpha=5\\%$, based on the standard normal approximation, the pairs bootstrap and the wild bootstrap using Radamacher weights. Use $1000$ Monte Carlo replications and $200$ bootstrap repetitions. The closer the empirical rejection rates are to $\\alpha$ (the nominal rate), the better the test performs. See the pdf of the problem set for a sketch of the coding structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0fe25dd-29fd-42bf-b598-a9f799580a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Import libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math as mt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "from sympy.stats import Rademacher, sample\n",
    "\n",
    "# 2) Set parameters for the data generating process\n",
    "n = 200\n",
    "\n",
    "# The desired mean values of v_i, z2_i and z3_i\n",
    "mu = np.array([0, 0, 0])\n",
    "\n",
    "# The desired covariance matrix for v_i, z2_i and z3_i\n",
    "cov = np.array([\n",
    "        [ 1,    0.2, 0.2],\n",
    "        [ 0.2,  1,   0.2],\n",
    "        [ 0.2,  0.2,   1]\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "739543f2-7fa1-4f72-90a1-76ce28b95254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define function for Monte Carlo simulation\n",
    "# r= number of MC replications, b= number of bootstrap repetitions, n= sample size, beta_1= parameter in null hypothesis \n",
    "\n",
    "def mysimul(r, b, n, rho_0, lambda_0, beta_1):\n",
    "    \n",
    "    # Preallocate zero vectors for rejection regions\n",
    "    RejN= np.zeros((r,1))\n",
    "    RejP= np.zeros((r,1))\n",
    "    RejW= np.zeros((r,1))\n",
    "    \n",
    "    for i in range(r):\n",
    "        \n",
    "        # 1) Generate general sample\n",
    "        rng = np.random.default_rng()\n",
    "        rv = rng.multivariate_normal(mu, cov, size=n)\n",
    "        v_i= rv[:,0]\n",
    "        z2_i= rv[:,1]\n",
    "        z3_i= rv[:,2]\n",
    "        # Generate z1_i according to the formula\n",
    "        z1_i= (np.exp(v_i) - mt.exp(0.5))/2.16\n",
    "        w1_i = np.random.normal(-1/9, 1, n)\n",
    "        w2_i = np.random.normal(1, 2, n) #mu, sigma, size\n",
    "        eta_i = np.random.binomial(size=n, n=1, p= 0.9)\n",
    "        # Generate e_i according to the formula\n",
    "        e_i= eta_i*w1_i + (1-eta_i)*w2_i\n",
    "        # Generate y1_i according to the formula\n",
    "        y_i= z1_i + z2_i + z3_i + rho_0*z1_i*z2_i + (1+lambda_0*z1_i)*e_i\n",
    "        # Collect regressors into a dataframe X\n",
    "        X = pd.DataFrame({'z1': z1_i, 'z2': z2_i, 'z3': z3_i })\n",
    "        # Add constant\n",
    "        X = sm.add_constant(X)\n",
    "        \n",
    "        # 2) Run regression on general sample\n",
    "        reg = sm.OLS(y_i, X).fit()\n",
    "        # Retrieve beta_1_hat from regression\n",
    "        beta_1_hat= reg.params[1]\n",
    "        # Retrieve standard error for beta_1_hat from regression\n",
    "        se= reg.HC0_se[1]\n",
    "        \n",
    "        # 3) Compute t-statistic for the general sample\n",
    "        tn= np.absolute((beta_1_hat - beta_1)/se)\n",
    "        \n",
    "        # 4) Generate  Bootstrap samples t-statistics\n",
    "        ## a) Pairs bootstrap\n",
    "        BootP0 = []\n",
    "        for _ in range(b):\n",
    "            # Create Pd dataframe with general sample for both y and Xs\n",
    "            mysample = pd.DataFrame({'y_i':y_i,'z1': z1_i, 'z2': z2_i, 'z3': z3_i })\n",
    "            # Create Pair Bootstrap sample\n",
    "            mysample_star= mysample.sample(n=200, frac=None, replace=True, weights=None, random_state=None, axis=None, ignore_index=False)\n",
    "            y_star = mysample_star['y_i']\n",
    "            X_star = mysample_star[['z1', 'z2', 'z3']]\n",
    "            # Add constant to X for the regression\n",
    "            X_star = sm.add_constant(X_star)\n",
    "            # Run regression\n",
    "            reg_BP = sm.OLS(y_star, X_star).fit()\n",
    "            # Retrieve beta_1_hat_BP from regression\n",
    "            beta_1_hat_BP= reg_BP.params[1]\n",
    "            # Retrieve standard error for beta_1_hat_BP from regression\n",
    "            se_BP= reg_BP.HC0_se[1]\n",
    "            # a.2) Compute t-statistic for Bootstrap-Pairs\n",
    "            tn_BP= (beta_1_hat_BP - beta_1_hat)/se_BP\n",
    "            # Store tn_BP values into BootP0\n",
    "            BootP0.append(tn_BP)\n",
    "        BootP= np.transpose(np.absolute(np.array(BootP0)))\n",
    "        \n",
    "        ## b) Wild bootstrap\n",
    "        BootW0 = []\n",
    "        ## For Wild bootstrap, I employ the CLS estimator beta_tilde as suggested in the notes (thanks to Wonjun Choi for the tip!)\n",
    "        y_new = y_i - beta_1*z1_i\n",
    "        X_new= pd.DataFrame({'z2': z2_i, 'z3': z3_i })\n",
    "        # Add constant\n",
    "        X_new=sm.add_constant(X_new)\n",
    "        # Run regression to obtain beta_tilde\n",
    "        reg_CLS=sm.OLS(y_new, X_new).fit()\n",
    "        beta_tilde= np.array(reg_CLS.params)\n",
    "        # Generate u_tilde according to the formula u_tilde= y_i - X*beta_tilde\n",
    "        u_tilde= y_i- np.matmul(X_new,beta_tilde)\n",
    "        # Generate Wild bootstrap sample\n",
    "        for _ in range(b):\n",
    "            # Generate w_i following Rademacher distribution\n",
    "            rad= Rademacher('W')\n",
    "            w_i = sample(rad, size= 200)\n",
    "            # Generate u_tilde_star according to the formula u_tilde_star_i= u_tilde_i*w_i\n",
    "            u_tilde_star = u_tilde*w_i\n",
    "            # Generate y_star2 according to the formula y_star2= X*beta_tilde + u_tilde_star\n",
    "            y_star2= np.matmul(X_new,beta_tilde) +z1_i + u_tilde_star\n",
    "            # Run regression\n",
    "            reg_BW = sm.OLS(y_star2, X).fit()\n",
    "            # Retrieve beta_1_hat_BW from regression\n",
    "            beta_1_hat_BW= reg_BW.params[1]\n",
    "            # Retrieve standard error for beta_1_hat_BW from regression\n",
    "            se_BW= reg_BW.HC0_se[1]\n",
    "            # Compute t-statistic for Bootstrap-Wild\n",
    "            tn_BW= (beta_1_hat_BW - beta_1)/se_BW\n",
    "            # Store tn_BW values into BootW0\n",
    "            BootW0.append(tn_BW)\n",
    "        BootW= np.transpose(np.absolute(np.array(BootW0)))\n",
    "        \n",
    "        # 5) Generate Bootstrap critical values\n",
    "        CriticalP= np.quantile(BootP,0.95)\n",
    "        CriticalW= np.quantile(BootW,0.95)\n",
    "        \n",
    "        # 6) Run t-test and store results into RejN, RejP, RejW\n",
    "        RejN[i] = 1*(tn > 1.96)\n",
    "        RejP[i] = 1*(tn > CriticalP)\n",
    "        RejW[i] = 1*(tn > CriticalW)\n",
    "    \n",
    "    # 7) Compute average rejection rate\n",
    "    RejN_mean= np.mean(RejN)\n",
    "    RejP_mean= np.mean(RejP)\n",
    "    RejW_mean= np.mean(RejW)\n",
    "    \n",
    "    print(\"The average rejection rate based on the standard normal approximation is\", RejN_mean)\n",
    "    print(\"The average rejection rate based on the Pair bootstrap is\", RejP_mean)\n",
    "    print(\"The average rejection rate based on the Wild bootstrap is\", RejW_mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72c7ff43-41ff-494e-a099-b27bbe57b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rejection rate based on the standard normal approximation is 0.171\n",
      "The average rejection rate based on the Pair bootstrap is 0.069\n",
      "The average rejection rate based on the Wild bootstrap is 0.052\n"
     ]
    }
   ],
   "source": [
    "# Run Montecarlo simulation with 1000 repetitions to test H0: beta_1=1\n",
    "mysimul(r=1000, b=200, n=200, rho_0=0, lambda_0=1, beta_1=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a440b433-cd1a-4ab3-b13b-e06acaffb567",
   "metadata": {},
   "source": [
    "(b) Repeat part (a) by setting $\\varrho_0\\in\\{-0.5,0.5\\}$ (so the model is misspecified), but now consider the two-sided $t$ test for $\\mathrm H_0: \\beta_1=1+\\varrho_0 b$ for $b=0.41$ (note that $1+\\varrho_0 b$ is the slope parameter for $Z_{1i}$ in the best linear prediction of $Y_i$ given $1,Z_{1i},Z_{2i}$, and $Z_{3i}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e451ab07-0753-4b8c-bab5-dbdf7de8d448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rejection rate based on the standard normal approximation is 0.182\n",
      "The average rejection rate based on the Pair bootstrap is 0.079\n",
      "The average rejection rate based on the Wild bootstrap is 0.037\n"
     ]
    }
   ],
   "source": [
    "# Set parameters for the misspecified model\n",
    "rho_0_mis= 0.5\n",
    "beta_1_mis= 1 + rho_0_mis*0.41\n",
    "# Run Montecarlo simulation\n",
    "mysimul(r=1000, b=200, n=200, rho_0=rho_0_mis, lambda_0=1, beta_1=beta_1_mis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8405d5-17d2-46bd-a416-1761d09c4d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average rejection rate based on the standard normal approximation is 0.179\n",
      "The average rejection rate based on the Pair bootstrap is 0.069\n",
      "The average rejection rate based on the Wild bootstrap is 0.026\n"
     ]
    }
   ],
   "source": [
    "# Repeat with rho_0_mis= -0.5\n",
    "rho_0_mis= -0.5\n",
    "beta_1_mis= 1 + rho_0_mis*0.41\n",
    "# Run Montecarlo simulation\n",
    "mysimul(r=1000, b=200, n=200, rho_0=rho_0_mis, lambda_0=1, beta_1=beta_1_mis)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m96",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m96"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
